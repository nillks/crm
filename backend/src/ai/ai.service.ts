import { Injectable, Logger, BadRequestException } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import { InjectRepository } from '@nestjs/typeorm';
import { Repository } from 'typeorm';
import Groq from 'groq-sdk';
import { GenerateAiResponseDto } from './dto';
import { AiSetting, AiProvider } from '../entities/ai-setting.entity';
import { AiLog } from '../entities/ai-log.entity';

@Injectable()
export class AIService {
  private readonly logger = new Logger(AIService.name);
  private groqClient: Groq;

  // –ú–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (Llama 3.1 8B —Ö–æ—Ä–æ—à–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä—É—Å—Å–∫–∏–π)
  private readonly DEFAULT_MODEL = 'llama-3.1-8b-instant';
  
  // –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å —Ö–æ—Ä–æ—à–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä—É—Å—Å–∫–æ–≥–æ:
  // - 'llama-3.1-8b-instant' (–±—ã—Å—Ç—Ä–∞—è, —Ö–æ—Ä–æ—à–∏–π —Ä—É—Å—Å–∫–∏–π)
  // - 'mixtral-8x7b-32768' (–±–æ–ª–µ–µ –º–æ—â–Ω–∞—è, –æ—Ç–ª–∏—á–Ω—ã–π —Ä—É—Å—Å–∫–∏–π)
  // - 'llama-3.1-70b-versatile' (—Å–∞–º–∞—è –º–æ—â–Ω–∞—è, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ)

  constructor(
    private configService: ConfigService,
    @InjectRepository(AiSetting)
    private aiSettingRepository: Repository<AiSetting>,
    @InjectRepository(AiLog)
    private aiLogRepository: Repository<AiLog>,
  ) {
    const apiKey = this.configService.get<string>('GROQ_API_KEY');
    
    if (!apiKey) {
      this.logger.warn('‚ö†Ô∏è GROQ_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. AI —Ñ—É–Ω–∫—Ü–∏–∏ –±—É–¥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –≤ mock —Ä–µ–∂–∏–º–µ.');
    } else {
      this.groqClient = new Groq({
        apiKey: apiKey,
      });
      this.logger.log('‚úÖ Groq AI –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω');
    }
  }

  /**
   * –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ "ChatGPT" endpoint
   * –ù–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Groq —Å Llama 3.1
   */
  async generateChatGPTResponse(dto: GenerateAiResponseDto): Promise<{
    response: string;
    tokensUsed: number;
    model: string;
    provider: string;
  }> {
    return this.generateResponse(dto, AiProvider.OPENAI, 'ChatGPT');
  }

  /**
   * –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ "Yandex GPT" endpoint
   * –ù–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Groq —Å Llama 3.1
   */
  async generateYandexGPTResponse(dto: GenerateAiResponseDto): Promise<{
    response: string;
    tokensUsed: number;
    model: string;
    provider: string;
  }> {
    return this.generateResponse(dto, AiProvider.YANDEX_GPT, 'Yandex GPT');
  }

  /**
   * –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞
   */
  private async generateResponse(
    dto: GenerateAiResponseDto,
    provider: AiProvider,
    providerName: string,
  ): Promise<{
    response: string;
    tokensUsed: number;
    model: string;
    provider: string;
  }> {
    // –û–±—ä—è–≤–ª—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤–Ω–µ try –±–ª–æ–∫–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ catch
    let aiSetting: AiSetting | null = null;
    let systemPrompt: string;
    let temperature: number;
    let maxTokens: number;
    let model: string;
    
    try {
      // –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ AI –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω clientId
      if (dto.clientId) {
        aiSetting = await this.aiSettingRepository.findOne({
          where: { clientId: dto.clientId },
        });

        if (aiSetting && !aiSetting.isEnabled) {
          throw new BadRequestException('AI –æ—Ç–∫–ª—é—á–µ–Ω –¥–ª—è —ç—Ç–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞');
        }
      }

      // –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
      systemPrompt = dto.systemPrompt || 
                     aiSetting?.systemPrompt || 
                     this.getDefaultSystemPrompt(providerName);
      temperature = dto.temperature ?? 
                    (aiSetting?.temperature ? parseFloat(aiSetting.temperature.toString()) : 0.7);
      maxTokens = dto.maxTokens ?? 
                  aiSetting?.maxTokens ?? 
                  1000;
      model = aiSetting?.model || this.DEFAULT_MODEL;

      // –ï—Å–ª–∏ API –∫–ª—é—á –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º mock –æ—Ç–≤–µ—Ç
      if (!this.groqClient) {
        this.logger.warn('‚ö†Ô∏è Groq API –∫–ª—é—á –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞—é mock –æ—Ç–≤–µ—Ç');
        const mockResponse = `[Mock ${providerName} Response] –≠—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–∞—à–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: "${dto.message}". –í production –∑–¥–µ—Å—å –±—É–¥–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç AI –º–æ–¥–µ–ª–∏.`;
        
        // –°–æ—Ö—Ä–∞–Ω—è–µ–º mock –ª–æ–≥ –≤ –ë–î
        const aiLog = this.aiLogRepository.create({
          clientId: dto.clientId || null,
          userId: dto.userId || null,
          provider: provider,
          model: 'mock',
          request: dto.message,
          response: mockResponse,
          systemPrompt: systemPrompt,
          tokensUsed: 0,
          temperature: temperature,
          maxTokens: maxTokens,
          metadata: { isMock: true },
          success: true,
        });
        await this.aiLogRepository.save(aiLog);
        
        return {
          response: mockResponse,
          tokensUsed: 0,
          model: 'mock',
          provider: providerName,
        };
      }

      // –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ Groq
      const messages = [
        {
          role: 'system' as const,
          content: systemPrompt,
        },
        {
          role: 'user' as const,
          content: dto.message,
        },
      ];

      this.logger.log(`ü§ñ –ó–∞–ø—Ä–æ—Å –∫ ${providerName} (Groq ${model}): ${dto.message.substring(0, 100)}...`);

      const completion = await this.groqClient.chat.completions.create({
        model: model,
        messages: messages,
        temperature: temperature,
        max_tokens: maxTokens,
      });

      const response = completion.choices[0]?.message?.content || '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç AI';
      const tokensUsed = completion.usage?.total_tokens || 0;
      const promptTokens = completion.usage?.prompt_tokens || 0;
      const completionTokens = completion.usage?.completion_tokens || 0;

      this.logger.log(`‚úÖ ${providerName} –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω (${tokensUsed} —Ç–æ–∫–µ–Ω–æ–≤)`);

      // –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö AI
      if (aiSetting) {
        aiSetting.tokensUsed = (aiSetting.tokensUsed || 0) + tokensUsed;
        await this.aiSettingRepository.save(aiSetting);
      }

      // –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥ –≤ –ë–î
      const aiLog = this.aiLogRepository.create({
        clientId: dto.clientId || null,
        userId: dto.userId || null,
        provider: provider,
        model: model,
        request: dto.message,
        response: response,
        systemPrompt: systemPrompt,
        tokensUsed: tokensUsed,
        temperature: temperature,
        maxTokens: maxTokens,
        metadata: {
          promptTokens,
          completionTokens,
          totalTokens: tokensUsed,
          finishReason: completion.choices[0]?.finish_reason,
        },
        success: true,
      });
      await this.aiLogRepository.save(aiLog);

      // –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∏ –æ—Ç–≤–µ—Ç
      this.logger.debug(`üìù ${providerName} –∑–∞–ø—Ä–æ—Å: ${dto.message}`);
      this.logger.debug(`üìù ${providerName} –æ—Ç–≤–µ—Ç: ${response.substring(0, 200)}...`);

      return {
        response,
        tokensUsed,
        model,
        provider: providerName,
      };
    } catch (error) {
      this.logger.error(`‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ ${providerName}:`, error);
      
      // –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥ –æ–± –æ—à–∏–±–∫–µ –≤ –ë–î
      try {
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–ª–∏ –ø–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        const errorModel = model || this.DEFAULT_MODEL;
        const errorSystemPrompt = systemPrompt || this.getDefaultSystemPrompt(providerName);
        const errorTemperature = temperature ?? 0.7;
        const errorMaxTokens = maxTokens ?? 1000;
        
        const aiLog = this.aiLogRepository.create({
          clientId: dto.clientId || null,
          userId: dto.userId || null,
          provider: provider,
          model: errorModel,
          request: dto.message,
          response: '',
          systemPrompt: errorSystemPrompt,
          tokensUsed: 0,
          temperature: errorTemperature,
          maxTokens: errorMaxTokens,
          metadata: { error: error.message },
          success: false,
          error: error.message || '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞',
        });
        await this.aiLogRepository.save(aiLog);
      } catch (logError) {
        this.logger.error(`‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ª–æ–≥–∞:`, logError);
      }
      
      if (error instanceof BadRequestException) {
        throw error;
      }

      throw new BadRequestException(
        `–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: ${error.message || '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞'}`,
      );
    }
  }

  /**
   * –ü–æ–ª—É—á–∏—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
   */
  private getDefaultSystemPrompt(providerName: string): string {
    if (providerName === 'Yandex GPT') {
      return `–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è CRM —Å–∏—Å—Ç–µ–º—ã. –¢—ã –ø–æ–º–æ–≥–∞–µ—à—å –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞–º –æ—Ç–≤–µ—á–∞—Ç—å –∫–ª–∏–µ–Ω—Ç–∞–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ, –≤–µ–∂–ª–∏–≤–æ –∏ –ø–æ –¥–µ–ª—É. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –±—É–¥—å –∫—Ä–∞—Ç–∫–∏–º, –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º.`;
    }
    
    return `You are a helpful AI assistant for a CRM system. You help operators respond to clients professionally, politely, and to the point. Respond in Russian, be concise but informative.`;
  }

  /**
   * –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AI
   */
  async getStats(clientId?: string): Promise<{
    totalRequests: number;
    totalTokens: number;
    clientsWithAI: number;
    successfulRequests: number;
    failedRequests: number;
  }> {
    // –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
    const settingsQuery = this.aiSettingRepository.createQueryBuilder('ai');
    if (clientId) {
      settingsQuery.where('ai.clientId = :clientId', { clientId });
    }
    const settings = await settingsQuery.getMany();
    
    // –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑ –ª–æ–≥–æ–≤
    const logsQuery = this.aiLogRepository.createQueryBuilder('log');
    if (clientId) {
      logsQuery.where('log.clientId = :clientId', { clientId });
    }
    const logs = await logsQuery.getMany();
    
    const successfulLogs = logs.filter(log => log.success);
    const failedLogs = logs.filter(log => !log.success);
    
    return {
      totalRequests: logs.length,
      totalTokens: logs.reduce((sum, log) => sum + (log.tokensUsed || 0), 0),
      clientsWithAI: settings.filter(s => s.isEnabled).length,
      successfulRequests: successfulLogs.length,
      failedRequests: failedLogs.length,
    };
  }

  /**
   * –ü–æ–ª—É—á–∏—Ç—å –ª–æ–≥–∏ AI –∑–∞–ø—Ä–æ—Å–æ–≤
   */
  async getLogs(params: {
    clientId?: string;
    userId?: string;
    provider?: AiProvider;
    limit?: number;
    offset?: number;
    startDate?: Date;
    endDate?: Date;
  }): Promise<{
    logs: AiLog[];
    total: number;
  }> {
    const query = this.aiLogRepository.createQueryBuilder('log');
    
    if (params.clientId) {
      query.andWhere('log.clientId = :clientId', { clientId: params.clientId });
    }
    
    if (params.userId) {
      query.andWhere('log.userId = :userId', { userId: params.userId });
    }
    
    if (params.provider) {
      query.andWhere('log.provider = :provider', { provider: params.provider });
    }
    
    if (params.startDate) {
      query.andWhere('log.createdAt >= :startDate', { startDate: params.startDate });
    }
    
    if (params.endDate) {
      query.andWhere('log.createdAt <= :endDate', { endDate: params.endDate });
    }
    
    query.orderBy('log.createdAt', 'DESC');
    
    const total = await query.getCount();
    
    if (params.limit) {
      query.limit(params.limit);
    }
    if (params.offset) {
      query.offset(params.offset);
    }
    
    const logs = await query.getMany();
    
    return {
      logs,
      total,
    };
  }
}

